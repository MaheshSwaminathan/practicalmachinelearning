---
title: "Practical Machine Learning course project"
output: html_document
---

##Summary

The goal of the project is to create a classification model to
identify the weight-training form in each observation as one of five possible categories A through E using some or all of 159 potential feature variables.

I use a Random Forest model with cross validation for this
project. This is based on project
[documentation](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md)
that points to a successful implementation of such a model.

I follow four key steps:

1.  Drop the first seven columns that are identifiers and do not contain predictive information on weight lifting form.
2.  Clean up inputs.
3.  Remove highly correlated features.
4.  Parallel implementation of Random Forest with 10-fold cross validation.

**Clean up inputs:** Drop data columns that contained NAs or "#DIV/0!" errors. Although other approaches are possible, I wanted to first try fitting a Reduced Feature Model based on potential advantages over imputing missing values (please see [Handling Missing Values when Applying Classification Models, Tsechansky and Provost](http://www.jmlr.org/papers/v8/saar-tsechansky07a.html) for a detailed discussion). This approach reduces the feature set by 100 columns. Since this approach results in a model with 99+% training accuracy, I did not pursue alternative data cleanup efforts.

**Remove highly correlated features:** Drop four highly correlated (cor > 0.9) features to improve efficiency. The final training data consists of 48 predictors and 14718 observations.

**RF with 10-fold CV:** Fit a Random Forest with 10-fold cross validation using 75% of the training data set. The selected model has 99.37% accuracy (average across 10 folds). This is the estimated out-of-sample accuracy of the model.

Applied to the 25% of observations held back for preliminary out-of-sample testing, the model's actual classification accuracy is 99.31%.

###R Code and Output

```{r eval=TRUE}

library(caret)
library(kernlab)
library(randomForest)

library(parallel)
library(doParallel)

#### Utility functions
anyError <- function(v) {
    return(anyNA(v)|any((v=="")|is.infinite(v)|(v=="#DIV/0!")))
}

predAccuracy <- function(a,b) {sum(a==b)/length(a)}


set.seed(97385)

training = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
testing = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")

### Drop indentifier columns
training153 = training[,-(1:7)]
testing153 = testing[,-(1:7)]

### Drop columns with bad data (missing, infinity, division by zero)
naCols <- apply(training153,2,anyError)
naVecInd <- which(as.matrix(naCols)[,1],arr.ind=TRUE)
nonErrTraining <- training153[,-naVecInd]
print(sprintf("Dropping %i columns with bad data", length(naVecInd)))
nonErrTesting <- testing153[,-naVecInd]

### Use 75% of the training data for training the model and hold back 25% for   out-of-sample testing partitions
smallPart <- createDataPartition(nonErrTraining$classe,p=0.75,list=FALSE)
smallPartTraining <- nonErrTraining[smallPart,]
### Complement of smallPartTraining, used for preliminary testing
compSmallPartTraining <- nonErrTraining[-smallPart,]

### Drop highly correlated columns, keep only one (the first)
corrMatrix <- cor(smallPartTraining[,-53])
highlyCorrelated <- findCorrelation(corrMatrix)[-1]
print(sprintf("Dropping %i columns with highly correlated (abs(cor) > 0.9) data", length(highlyCorrelated)))
reducedFeaturesTraining <- smallPartTraining[,-highlyCorrelated]
## Clean up the prelim test and final test predictor columns too
reducedFeaturesPrelimTesting <- compSmallPartTraining[,-highlyCorrelated]
reducedFeaturesTesting <- nonErrTesting[,-highlyCorrelated]


### Run fitting in parallel threads
### Parallelization code snippet courtesy https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
fitControl <- trainControl(method = "cv", number = 10, allowParallel = TRUE)
print(sprintf("Training start: %s", date()))
fit <- train(classe ~ .,method="rf",data=reducedFeaturesTraining,trControl = fitControl)
print(sprintf("Training end: %s", date()))
stopCluster(cluster)
registerDoSEQ()


print(fit)
print(fit$resample)
print(confusionMatrix.train(fit))

### Out-of-sample testing
## Prelim test
predPrelimTest <- predict(fit, newdata=reducedFeaturesPrelimTesting)
cat(sprintf("Prelim testing accuracy:\t%f\n", predAccuracy(predPrelimTest, reducedFeaturesPrelimTesting$classe)))

### Turn-off printing predictions on the actual test data provided
##predTest <- predict(fit, newdata=reducedFeaturesTesting)
##cat(sprintf("ProblemID\tPredTest\n"))
##cat(sprintf("%i\t%s",reducedFeaturesTesting$problem_id, predTest))



```
